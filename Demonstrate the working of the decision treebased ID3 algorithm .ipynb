import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
iris_df = pd.read_csv('iris.csv')
X = iris_df.iloc[:, :-1]
y = iris_df.iloc[:, -1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
def id3(X, y):
 if y.nunique() == 1:
 return y.iloc[0]
 if X.shape[1] == 0:
 return y.value_counts().idxmax()
 information_gains = []
 for attribute in X.columns:
 entropy_parent = entropy(y)
 unique_values = X[attribute].unique()
 entropy_children = 0
 for value in unique_values:
 y_subset = y[X[attribute] == value]
 entropy_children += len(y_subset) / len(y) * entropy(y_subset)
 information_gain = entropy_parent - entropy_children
 information_gains.append(information_gain)
 best_attribute = X.columns[np.argmax(information_gains)]
 node = {'attribute': best_attribute, 'children': {}}
 unique_values = X[best_attribute].unique()
 for value in unique_values:
 X_subset = X[X[best_attribute] == value].drop(best_attribute, axis=1)
 y_subset = y[X[best_attribute] == value]
 node['children'][value] = id3(X_subset, y_subset)
 return node
def entropy(y):
 p = y.value_counts(normalize=True)
 entropy = -np.sum(p * np.log2(p))
 return entropy
def predict(node, x):
 if 'children' not in node:
 return node
 attribute = node['attribute']
 value = x[attribute]
 if value not in node['children']:
 return None
 child = node['children'][value]
 return predict(child, x)
decision_tree = id3(X_train, y_train)
y_pred = X_test.apply(lambda x: predict(decision_tree, x), axis=1)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of decision tree:", accuracy)
for i in range(len(y_pred)):
 if y_pred.iloc[i] == y_test.iloc[i]:
 print("Sample:", X_test.iloc[i].values, "Predicted class:", y_pred.iloc[i], "True class:",
y_test.iloc[i], "Correct")
 else:
 print("Sample:", X_test.iloc[i].values, "Predicted class:", y_pred.iloc[i], "True class:",
y_test.iloc[i], "Wrong")
